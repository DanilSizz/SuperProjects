- #КолмогоровскаяСложность - #СжатиеДанных - #КомпьютернаяНаука #информатика




Колмогоровская сложность - это мера сложности алгоритма, предложенная Андреем
Колмогоровым в 1960-х годах. Она измеряет количество информации, необходимое для описания объекта или алгоритма.  
   
Простыми словами, колмогоровская сложность - это количество бит, необходимое для записи алгоритма или объекта в компьютере. Чем меньше бит нужно, тем проще алгоритм или объект.  
   
Формула для колмогоровской сложности выглядит следующим образом:  
   
К(x) = min{l(p): U(p) = x}  
   
Где:  
   
К(x) - колмогоровская сложность объекта x  
p - программа, которая выводит объект x  
l(p) - длина программы p в битах  
U(p) - результат выполнения программы p  
В этой формуле мы ищем программу p, которая выводит объект x и имеет минимальную длину l(p). Это и есть колмогоровская сложность объекта x.  
   
Например, если у нас есть строка "Hello, world!", то колмогоровская сложность этой строки будет равна количеству бит, необходимому для записи программы, которая выводит эту строку. Если мы можем записать программу, которая выводит эту строку, используя 100 бит, то колмогоровская сложность строки будет равна 100.  
   
Важно отметить, что колмогоровская сложность - это теоретическая мера сложности, и она не всегда может быть вычислена точно. Однако, она дает нам представление о сложности алгоритмов и объектов и помогает в сравнении их сложности.10:59

Message


Шенновская энтропия - это мера неопределенности или случайности в информации. Она была предложена Клодом Шенноном в 1948 году и является основой теории информации.  
   
Простыми словами, Шенновская энтропия - это количество информации, которое мы получаем, когда узнаем значение случайной величины. Чем больше энтропия, тем больше информации мы получаем, и тем меньше мы знаем о значении этой величины заранее.  
   
Формула для Шенновской энтропии выглядит следующим образом:  
   
H(X) = - ∑ p(x) * log2 p(x)  
   
Где:  
   
H(X) - Шенновская энтропия случайной величины X  
p(x) - вероятность значения x  
log2 - логарифм по основанию 2  
В этой формуле мы суммируем произведения вероятностей и логарифмов вероятностей для каждого возможного значения случайной величины X. Результатом является мера неопределенности или энтропии этой величины.  
   
Например, если у нас есть монета, которая может выпасть либо орлом, либо решкой, то Шенновская энтропия этой монеты будет равна 1, потому что мы получаем 1 бит информации, когда узнаем результат броска монеты. Если бы монета могла выпасть одним из четырех значений, то Шенновская энтропия была бы равна 2, потому что мы получаем 2 бита информации.  
   
Шенновская энтропия имеет важное значение в теории информации, потому что она позволяет измерять количество информации, которое передается через канал связи, и определять пределы сжатия данных.11:09

![](blob:https://web.telegram.org/cf5f9a1f-e992-4615-8949-19eedacab4a9)