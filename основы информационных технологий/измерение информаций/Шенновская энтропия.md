#ИзмерениеиИнформации #информатика #информацияонааятеория #шенновскаяэнтропия 
[[шеннон клод]]
Шенновская энтропия - это мера неопределенности или случайности в информации. Она была предложена Клодом Шенноном в 1948 году и является основой теории информации.

Простыми словами, Шенновская энтропия - это количество информации, которое мы получаем, когда узнаем значение случайной величины. Чем больше энтропия, тем больше информации мы получаем, и тем меньше мы знаем о значении этой величины заранее.

Формула для Шенновской энтропии выглядит следующим образом:

H(X) = - ∑ p(x) * log2 p(x)

Где:

- H(X) - Шенновская энтропия случайной величины X
- p(x) - вероятность значения x
- log2 - логарифм по основанию 2

В этой формуле мы суммируем произведения вероятностей и логарифмов вероятностей для каждого возможного значения случайной величины X. Результатом является мера неопределенности или энтропии этой величины.

Например, если у нас есть монета, которая может выпасть либо орлом, либо решкой, то Шенновская энтропия этой монеты будет равна 1, потому что мы получаем 1 бит информации, когда узнаем результат броска монеты. Если бы монета могла выпасть одним из четырех значений, то Шенновская энтропия была бы равна 2, потому что мы получаем 2 бита информации.

Шенновская энтропия имеет важное значение в теории информации, потому что она позволяет измерять количество информации, которое передается через канал связи, и определять пределы сжатия данных.