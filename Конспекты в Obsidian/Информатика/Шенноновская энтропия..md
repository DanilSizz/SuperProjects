#АлгоритмыИнформация #инфоматика #КолмогоровИИнформация #КолмогоровскаяСложность 
Энтропия Шеннона — фундаментальное понятие в теории информации, введенное Клодом Шенноном в 1940-х годах. Она измеряет степень неопределенности или случайности в распределении вероятностей.

Вот ключевые моменты Шенноновской энтропии:

- Это мера среднего количества информации, необходимого для определения результата в распределении вероятностей.
- Обычно обозначается символом H и измеряется в единицах
- Формула энтропии Шеннона выглядит так: H = - ∑ p(x) * log2(p(x)), где p(x) — вероятность каждого результата.
- Он обладает рядом важных свойств, в том числе:
    - Неотрицательность: H ≥ 0
    - Симметрия: H инвариантен относительно перестановок результатов.
    - Масштабируемость: H(αX) = αH(X) для любого α >
    - Субаддитивность: H(X, Y) ≤ H(X) + H(Y)
- Энтропия Шеннона имеет множество применений в:
    - Сжатие данных: обеспечивает нижнюю границу количества бит, необходимых для кодирования сообщения.
    - Коды исправления ошибок: определяют максимальную скорость, с которой информация может надежно передаваться по каналу связи.
    - Криптография: используется для анализа безопасности шифрования.
    - Машинное обучение: используется как мера неопределенности