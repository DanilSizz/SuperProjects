#Случайность #Информация #Энтропия #Сложность_строк #Байесовский_принцип #Минимизация_описания #Статистическая_связность 

Шеннон энтропия - это мера неопределенности или случайности в системе, которая была введена Клодом Шенноном в 1948 году. Энтропия является фундаментальной концепцией в теории информации и широко используется в различных областях, таких как криптография, кодирование и передача информации.

Формула Шеннон энтропии:

H(X) = - ∑ p(x) log2 p(x)

где:

- H(X) - энтропия системы X
- p(x) - вероятность каждого возможного значения x в системе X
- log2 - логарифм по основанию 2

Эта формула показывает, что энтропия системы является суммой произведений вероятностей каждого возможного значения и логарифма этих вероятностей.

Например, если у нас есть система с двумя возможными значениями, A и B, с вероятностями 0,7 и 0,3 соответственно, то энтропия системы будет:

H(X) = - (0,7 log2 0,7 + 0,3 log2 0,3) ≈ 0,88

Это означает, что система имеет относительно высокую неопределенность или случайность.

Шеннон энтропия имеет следующие свойства:

- Энтропия системы неотрицательна (H(X) ≥ 0)
- Энтропия системы равна нулю, если система определена (H(X) = 0)
- Энтропия системы максимизируется, когда все возможные значения имеют равные вероятности (H(X) = log2 n, где n - количество возможных значений)

Шеннон энтропия используется в различных приложениях, таких как:

- Кодирование и декодирование информации
- Криптография и шифрование
- Анализ данных и машинное обучение
- Биоинформатика и геномика

В частности, Шеннон энтропия используется для оценки эффективности кодирования и передачи информации, а также для определения сложности систем и процессов.